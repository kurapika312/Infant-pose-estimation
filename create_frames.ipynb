{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fake ground truth (for validation mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test1.jpg', 'test1879.jpg', 'test4160.jpg']\n",
      "0 test1.jpg\n",
      "1 test1879.jpg\n",
      "2 test4160.jpg\n"
     ]
    }
   ],
   "source": [
    "folder_name = 'data/syrip/images/validate_infant'\n",
    "image_list = []\n",
    "annotation_list = []\n",
    "print(os.listdir(folder_name))\n",
    "\n",
    "is_synthetic = False\n",
    "\n",
    "is_labeled = True\n",
    "image_source = \"https://www.youtube.com/watch?v=5q-wXlKSxD8\"\n",
    "\n",
    "bbox_head = []\n",
    "category_id = 1 \n",
    "nframes = 1\n",
    "scores = []\n",
    "num_keypoints = 17\n",
    "segmentation = []\n",
    "area = 100\n",
    "iscrowd = 0\n",
    "keypoints = [\n",
    "    10, 10, 2.0,\n",
    "    11, 11, 2.0, \n",
    "    12, 12, 2.0, \n",
    "    13, 13, 2.0, \n",
    "    14, 14, 2.0, \n",
    "    15, 15, 2.0, \n",
    "    16, 16, 2.0,\n",
    "    17, 17, 2.0, \n",
    "    18, 18, 2.0, \n",
    "    19, 19, 2.0, \n",
    "    20, 20, 2.0, \n",
    "    21, 21, 2.0, \n",
    "    22, 22, 2.0, \n",
    "    23, 23, 2.0, \n",
    "    24, 24, 2.0, \n",
    "    25, 25, 2.0, \n",
    "    26, 26, 2.0\n",
    "    ]\n",
    "\n",
    "for i, file_name in enumerate(sorted(os.listdir(folder_name))):\n",
    "    print(i, file_name)\n",
    "    img = cv2.imread(folder_name + \"/\" + file_name)\n",
    "    height, width, _ = img.shape\n",
    "    bbox = [1, 1, height, width]\n",
    "    original_file_name = file_name\n",
    "    index = int(''.join([n for n in file_name if n.isdigit()]))\n",
    "    frame_id = index\n",
    "    id = index\n",
    "    track_id = index\n",
    "    image_id = index\n",
    "    \n",
    "    image_dict = {\n",
    "        \"file_name\": file_name,\n",
    "        \"is_synthetic\": is_synthetic,\n",
    "        \"frame_id\": frame_id, \n",
    "        \"height\": height,\n",
    "        \"id\": id,\n",
    "        \"is_labeled\": is_labeled,\n",
    "        \"nframes\": nframes,\n",
    "        \"original_file_name\": original_file_name,\n",
    "        \"width\": width, \n",
    "        \"image_source\": image_source\n",
    "        }\n",
    "    image_list.append(image_dict)    \n",
    "    \n",
    "    annotation_dict = {\n",
    "        \"bbox\": bbox,\n",
    "        \"bbox_head\": bbox_head,\n",
    "        \"category_id\": category_id,\n",
    "        \"id\": id,\n",
    "        \"image_id\": image_id,\n",
    "        \"keypoints\": keypoints,\n",
    "        \"scores\": scores,\n",
    "        \"track_id\": track_id,\n",
    "        \"num_keypoints\": num_keypoints, \n",
    "        \"segmentation\": segmentation,\n",
    "        \"area\": area,\n",
    "        \"iscrowd\": iscrowd\n",
    "        } \n",
    "    annotation_list.append(annotation_dict)\n",
    "\n",
    "category_list = [{\"supercategory\": \"person\", \"id\": 1, \"name\": \"person\", \"keypoints\": [\"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\", \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\", \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\", \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"], \"skeleton\": [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], [6, 7], [6, 8], [7, 9], [8, 10], [9, 11], [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]]}]\n",
    "content = {\n",
    "    \"images\": image_list, \n",
    "    \"annotations\": annotation_list,\n",
    "    \"categories\": category_list       \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_object = json.dumps(content, indent = 4)\n",
    "with open(\"data/syrip/annotations/person_keypoints_validate_infant.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Namespace(cfg='experiments/coco/hrnet/w48_384x288_adam_lr1e-3_infant.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/hrnet_fidip.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n",
      "Namespace(cfg='experiments/coco/hrnet/w48_384x288_adam_lr1e-3_infant.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/hrnet_fidip.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n",
      "Namespace(cfg='experiments/coco/hrnet/w48_384x288_adam_lr1e-3_infant.yaml', dataDir='', logDir='', modelDir='', opts=['TEST.MODEL_FILE', 'models/hrnet_fidip.pth', 'TEST.USE_GT_BBOX', 'True'], prevModelDir='')\n",
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: syrip\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/syrip/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: validate_infant\n",
      "  TRAIN_PRE_SET: train_pre_infant\n",
      "  TRAIN_SET: train_infant\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: True\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  EXTRA:\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "    STAGE2:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4]\n",
      "      NUM_BRANCHES: 2\n",
      "      NUM_CHANNELS: [48, 96]\n",
      "      NUM_MODULES: 1\n",
      "    STAGE3:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4]\n",
      "      NUM_BRANCHES: 3\n",
      "      NUM_CHANNELS: [48, 96, 192]\n",
      "      NUM_MODULES: 4\n",
      "    STAGE4:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4, 4]\n",
      "      NUM_BRANCHES: 4\n",
      "      NUM_CHANNELS: [48, 96, 192, 384]\n",
      "      NUM_MODULES: 3\n",
      "  HEATMAP_SIZE: [72, 96]\n",
      "  IMAGE_SIZE: [288, 384]\n",
      "  INIT_WEIGHTS: True\n",
      "  NAME: adaptive_pose_hrnet\n",
      "  NUM_JOINTS: 17\n",
      "  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n",
      "  SIGMA: 3\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 20\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 16\n",
      "  BBOX_THRE: 1.0\n",
      "  BLUR_KERNEL: 11\n",
      "  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: models/hrnet_fidip.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 20\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 20\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LAMBDA: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_FACTOR: 0.1\n",
      "  LR_STEP: [40, 200]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  PRE_BATCH_SIZE_PER_GPU: 16\n",
      "  PRE_EPOCH: 0\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 1\n",
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: syrip\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/syrip/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: validate_infant\n",
      "  TRAIN_PRE_SET: train_pre_infant\n",
      "  TRAIN_SET: train_infant\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: True\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  EXTRA:\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "    STAGE2:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4]\n",
      "      NUM_BRANCHES: 2\n",
      "      NUM_CHANNELS: [48, 96]\n",
      "      NUM_MODULES: 1\n",
      "    STAGE3:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4]\n",
      "      NUM_BRANCHES: 3\n",
      "      NUM_CHANNELS: [48, 96, 192]\n",
      "      NUM_MODULES: 4\n",
      "    STAGE4:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4, 4]\n",
      "      NUM_BRANCHES: 4\n",
      "      NUM_CHANNELS: [48, 96, 192, 384]\n",
      "      NUM_MODULES: 3\n",
      "  HEATMAP_SIZE: [72, 96]\n",
      "  IMAGE_SIZE: [288, 384]\n",
      "  INIT_WEIGHTS: True\n",
      "  NAME: adaptive_pose_hrnet\n",
      "  NUM_JOINTS: 17\n",
      "  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n",
      "  SIGMA: 3\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 20\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 16\n",
      "  BBOX_THRE: 1.0\n",
      "  BLUR_KERNEL: 11\n",
      "  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: models/hrnet_fidip.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 20\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 20\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LAMBDA: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_FACTOR: 0.1\n",
      "  LR_STEP: [40, 200]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  PRE_BATCH_SIZE_PER_GPU: 16\n",
      "  PRE_EPOCH: 0\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 1\n",
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: syrip\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/syrip/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: validate_infant\n",
      "  TRAIN_PRE_SET: train_pre_infant\n",
      "  TRAIN_SET: train_infant\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: True\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  EXTRA:\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    PRETRAINED_LAYERS: ['conv1', 'bn1', 'conv2', 'bn2', 'layer1', 'transition1', 'stage2', 'transition2', 'stage3', 'transition3', 'stage4']\n",
      "    STAGE2:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4]\n",
      "      NUM_BRANCHES: 2\n",
      "      NUM_CHANNELS: [48, 96]\n",
      "      NUM_MODULES: 1\n",
      "    STAGE3:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4]\n",
      "      NUM_BRANCHES: 3\n",
      "      NUM_CHANNELS: [48, 96, 192]\n",
      "      NUM_MODULES: 4\n",
      "    STAGE4:\n",
      "      BLOCK: BASIC\n",
      "      FUSE_METHOD: SUM\n",
      "      NUM_BLOCKS: [4, 4, 4, 4]\n",
      "      NUM_BRANCHES: 4\n",
      "      NUM_CHANNELS: [48, 96, 192, 384]\n",
      "      NUM_MODULES: 3\n",
      "  HEATMAP_SIZE: [72, 96]\n",
      "  IMAGE_SIZE: [288, 384]\n",
      "  INIT_WEIGHTS: True\n",
      "  NAME: adaptive_pose_hrnet\n",
      "  NUM_JOINTS: 17\n",
      "  PRETRAINED: models/pytorch/imagenet/hrnet_w48-8ef0771d.pth\n",
      "  SIGMA: 3\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 20\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 16\n",
      "  BBOX_THRE: 1.0\n",
      "  BLUR_KERNEL: 11\n",
      "  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: models/hrnet_fidip.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 20\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 20\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LAMBDA: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_FACTOR: 0.1\n",
      "  LR_STEP: [40, 200]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  PRE_BATCH_SIZE_PER_GPU: 16\n",
      "  PRE_EPOCH: 0\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating output/syrip/adaptive_pose_hrnet/w48_384x288_adam_lr1e-3_infant\n",
      "=> creating log/syrip/adaptive_pose_hrnet/w48_384x288_adam_lr1e-3_infant_2023-04-05-01-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> loading model from models/hrnet_fidip.pth\n",
      "=> loading model from models/hrnet_fidip.pth\n",
      "=> loading model from models/hrnet_fidip.pth\n",
      "=> classes: ['__background__', 'person']\n",
      "=> classes: ['__background__', 'person']\n",
      "=> classes: ['__background__', 'person']\n",
      "=> num_images: 3\n",
      "=> num_images: 3\n",
      "=> num_images: 3\n",
      "=> load 3 samples\n",
      "=> load 3 samples\n",
      "=> load 3 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: [0/1]\tTime 0.178 (0.178)\tLoss 0.0038 (0.0038)\tAccuracy 0.000 (0.000)\n",
      "Test: [0/1]\tTime 0.178 (0.178)\tLoss 0.0038 (0.0038)\tAccuracy 0.000 (0.000)\n",
      "Test: [0/1]\tTime 0.178 (0.178)\tLoss 0.0038 (0.0038)\tAccuracy 0.000 (0.000)\n",
      "=> writing results json to output/syrip/adaptive_pose_hrnet/w48_384x288_adam_lr1e-3_infant/results/keypoints_validate_infant_results_0.json\n",
      "=> writing results json to output/syrip/adaptive_pose_hrnet/w48_384x288_adam_lr1e-3_infant/results/keypoints_validate_infant_results_0.json\n",
      "=> writing results json to output/syrip/adaptive_pose_hrnet/w48_384x288_adam_lr1e-3_infant/results/keypoints_validate_infant_results_0.json\n",
      "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
      "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
      "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
      "|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|\n",
      "| adaptive... | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 |\n",
      "| adaptive... | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 |\n",
      "| adaptive... | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 | 0.000 | 0.000 | 0.000 | -1.000 | -1.000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1879\n",
      "4160\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "%run tools/test_adaptive_model.py \\\n",
    "    --cfg experiments/coco/hrnet/w48_384x288_adam_lr1e-3_infant.yaml \\\n",
    "    TEST.MODEL_FILE models/hrnet_fidip.pth TEST.USE_GT_BBOX True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The resulted .json file for keypoints is split into .json files each corresponding to one input image. Also, the format of the keypoints should be modified to match the SMPLify-X keypoints format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Data/ETS-Projects/Current-work/Infant-Pose-Estimation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "%cd /media/Data/ETS-Projects/Current-work/Infant-Pose-Estimation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mcfg_files\u001b[0m/      \u001b[34;42mmodels\u001b[0m/                     \u001b[34;42mpriors\u001b[0m/     \u001b[01;32mrequirements.txt\u001b[0m*\n",
      "\u001b[34;42mdata\u001b[0m/           \u001b[01;32moptional-requirements.txt\u001b[0m*  \u001b[01;32mREADME.md\u001b[0m*  \u001b[01;32mSMIL_fitting.ipynb\u001b[0m*\n",
      "\u001b[01;32mload_prior.py\u001b[0m*  \u001b[34;42moutput\u001b[0m/                     \u001b[34;42mrender\u001b[0m/     \u001b[34;42msmplifyx\u001b[0m/\n",
      "\u001b[0m\u001b[34;42mcfg_files\u001b[0m/      \u001b[34;42mmodels\u001b[0m/                     \u001b[34;42mpriors\u001b[0m/     \u001b[01;32mrequirements.txt\u001b[0m*\n",
      "\u001b[34;42mdata\u001b[0m/           \u001b[01;32moptional-requirements.txt\u001b[0m*  \u001b[01;32mREADME.md\u001b[0m*  \u001b[01;32mSMIL_fitting.ipynb\u001b[0m*\n",
      "\u001b[01;32mload_prior.py\u001b[0m*  \u001b[34;42moutput\u001b[0m/                     \u001b[34;42mrender\u001b[0m/     \u001b[34;42msmplifyx\u001b[0m/\n",
      "\u001b[0m\u001b[34;42mcfg_files\u001b[0m/      \u001b[34;42mmodels\u001b[0m/                     \u001b[34;42mpriors\u001b[0m/     \u001b[01;32mrequirements.txt\u001b[0m*\n",
      "\u001b[34;42mdata\u001b[0m/           \u001b[01;32moptional-requirements.txt\u001b[0m*  \u001b[01;32mREADME.md\u001b[0m*  \u001b[01;32mSMIL_fitting.ipynb\u001b[0m*\n",
      "\u001b[01;32mload_prior.py\u001b[0m*  \u001b[34;42moutput\u001b[0m/                     \u001b[34;42mrender\u001b[0m/     \u001b[34;42msmplifyx\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "results_folder = 'output/syrip/adaptive_pose_hrnet/w48_384x288_adam_lr1e-3_infant/results/'\n",
    "f = open(results_folder + 'keypoints_validate_infant_results_0.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "syrip_joint_names = [\"nose\", \n",
    "                    \"left_eye\", \n",
    "                    \"right_eye\", \n",
    "                    \"left_ear\", \n",
    "                    \"right_ear\", \n",
    "                    \"left_shoulder\", \n",
    "                    \"right_shoulder\", \n",
    "                    \"left_elbow\", \n",
    "                    \"right_elbow\", \n",
    "                    \"left_wrist\", \n",
    "                    \"right_wrist\",\n",
    "                    \"left_hip\", \n",
    "                    \"right_hip\", \n",
    "                    \"left_knee\", \n",
    "                    \"right_knee\", \n",
    "                    \"left_ankle\", \n",
    "                    \"right_ankle\"] \n",
    "missing_joint_names = [\"neck\", \"pelvis\"]\n",
    "syrip_joint_names_faked = syrip_joint_names\n",
    "syrip_joint_names_faked.extend(missing_joint_names)\n",
    "syrip_joint_dict_faked = dict(zip(syrip_joint_names_faked, range(len(syrip_joint_names_faked))))\n",
    "\n",
    "coco19_joint_names = [\"nose\",\n",
    "                    \"neck\",\n",
    "                    \"right_shoulder\",\n",
    "                    \"right_elbow\",\n",
    "                    \"right_wrist\",\n",
    "                    \"left_shoulder\",\n",
    "                    \"left_elbow\",\n",
    "                    \"left_wrist\",\n",
    "                    \"pelvis\",\n",
    "                    \"right_hip\",\n",
    "                    \"right_knee\",\n",
    "                    \"right_ankle\",\n",
    "                    \"left_hip\",\n",
    "                    \"left_knee\",\n",
    "                    \"left_ankle\",\n",
    "                    \"right_eye\",\n",
    "                    \"left_eye\",\n",
    "                    \"right_ear\",\n",
    "                    \"left_ear\",] \n",
    "coco19_joint_dict = dict(zip(coco19_joint_names, range(len(coco19_joint_names))))\n",
    "\n",
    "joint_mapping = {}\n",
    "for key in syrip_joint_dict_faked:\n",
    "    if key in coco19_joint_dict:\n",
    "        joint_mapping[key] = (syrip_joint_dict_faked[key],coco19_joint_dict[key])\n",
    "        \n",
    "import re  \n",
    "import shutil    \n",
    "\n",
    "def find_files(directory, number):\n",
    "    regex = re.compile(r'(^|\\D){}($|\\D)'.format(number))\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if regex.search(filename):\n",
    "                yield os.path.join(root, filename) \n",
    "        \n",
    "target_keypoints_folder = 'syn_generation/data/keypoints/'\n",
    "target_images_folder = 'syn_generation/data/images/'\n",
    "input_image_folder = 'data/syrip/images/validate_infant'\n",
    "for temp in data:\n",
    "    keypoint_syrip = temp['keypoints']\n",
    "    keypoint_syrip_faked = keypoint_syrip\n",
    "    keypoint_syrip_faked.extend([0]*3*len(missing_joint_names))\n",
    "    keypoint_coco19 = [0]*len(keypoint_syrip_faked)\n",
    "    for key in joint_mapping:\n",
    "        source_joint_index, target_joint_index = joint_mapping[key]\n",
    "        keypoint_coco19[3*target_joint_index:3*target_joint_index+3] = keypoint_syrip_faked[3*source_joint_index:3*source_joint_index+3]\n",
    "        content = {\"annotations\":keypoint_coco19}\n",
    "        json_object = json.dumps(content, indent = 4)\n",
    "    for filename in find_files(input_image_folder, str(temp['image_id'])):\n",
    "        shutil.copy(filename, target_images_folder)\n",
    "        with open(target_keypoints_folder + filename.replace(input_image_folder,'').replace('.jpg','') +\"_keypoints.json\", \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "            \n",
    "        os.chdir('syn_generation')\n",
    "\n",
    "        %run smplifyx/main.py --config cfg_files/fit_smil.yaml --data_folder data --output_folder output --visualize=True --model_folder models\n",
    "        \n",
    "        os.chdir(\"..\")\n",
    "        \n",
    "        for image_filename in find_files(target_images_folder, str(temp['image_id'])):\n",
    "            os.remove(image_filename)\n",
    "        for keypoint_filename in find_files(target_keypoints_folder, str(temp['image_id'])):\n",
    "            os.remove(keypoint_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syrip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ff1fb173ab756182516332e063e4fb28ef14e8b6103d3f3fcfcf0ef63c421a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
