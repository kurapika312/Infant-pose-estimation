# -*- coding: utf-8 -*-
"""Pediatrics-Segmentation-Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y04sTpG9NrkaQCgdfbo_aJzLFEjcxlCf

# Detectron2 Beginner's Tutorial

<img src="https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png" width="500">

Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:
* Run inference on images or videos, with an existing detectron2 model
* Train a detectron2 model on a new dataset

You can make a copy of this tutorial by "File -> Open in playground mode" and make changes there. __DO NOT__ request access to this tutorial.

# Install detectron2
"""
import pathlib
import tqdm
import torch, detectron2

TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.structures import Boxes

# import pathlib
# import matplotlib.pyplot as plt

# import mediapipe as mp

# from mediapipe.tasks import python
# from mediapipe.tasks.python import vision


"""# Run a pre-trained detectron2 model

!python -m pip install pyyaml==5.1
import sys, os, distutils.core
# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.
# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions
!git clone 'https://github.com/facebookresearch/detectron2'
dist = distutils.core.run_setup("./detectron2/setup.py")
!python -m pip install {' '.join([f"'{x}'" for x in dist.install_requires])}
sys.path.insert(0, os.path.abspath('./detectron2'))

# Properly install detectron2. (Please do not install twice in both ways)
# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

We first download an image from the COCO dataset:

Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image.
"""

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
# outputs = predictor(im)


## Function for detecting and segmenting only 1 class i.e cups
def onlykeep_person_class(oim, outputs):
  cls = outputs['instances'].pred_classes
  scores = outputs["instances"].scores
  boxes = outputs['instances'].pred_boxes
  masks = outputs['instances'].pred_masks

  # index to keep whose class == 0
  indx_to_keep = (cls == 0).nonzero().flatten().tolist()
    
  # only keeping index  corresponding arrays
  cls1 = torch.tensor(np.take(cls.cpu().numpy(), indx_to_keep))
  scores1 = torch.tensor(np.take(scores.cpu().numpy(), indx_to_keep))
  boxes1 = Boxes(torch.tensor(np.take(boxes.tensor.cpu().numpy(), indx_to_keep, axis=0)))
  masks1 = torch.tensor(np.take(masks.cpu().numpy(), indx_to_keep, axis=0))
  masks = masks1.clone().squeeze(0).numpy() # More than 1 masks can be returned

  np_mask = np.zeros(oim.shape)
  more_than_one_mask = len(masks.shape) > 2
  if(more_than_one_mask):
    for i in range(masks.shape[0]):
      np_mask[masks[i]] = 255
      break
  else:
    np_mask[masks] = 255
  
  # create new instance obj and set its fields
  obj = detectron2.structures.Instances(image_size=(oim.shape[0], oim.shape[1]))
  obj.set('pred_classes', cls1)
  obj.set('scores', scores1)
  obj.set('pred_boxes',boxes1)
  obj.set('pred_masks',masks1)
  return obj, np_mask, more_than_one_mask

images_dir = pathlib.Path('assets/Ashok-RGB2D/RGB')
images_save_dir = pathlib.Path('assets/Ashok-RGB2D/SEGMENTATION')
images_glob = images_dir.glob('*.png')

images_save_dir.mkdir(exist_ok=True, parents=True)

# Create the options that will be used for ImageSegmenter
# base_options = python.BaseOptions(model_asset_path='./models/mediapipe/segmentation/selfie_segmenter_landscape.tflite')
# options = vision.ImageSegmenterOptions(base_options=base_options,
#                                        output_category_mask=True)
# BG_COLOR = (0, 0, 0) # gray
# MASK_COLOR = (255, 255, 255) # white

for im_path in tqdm.tqdm(images_glob):
  im_segmentation_name = f'{im_path.stem.split("-")[0]}-SEGMENTATION'
  im_segmentation_path = f'{images_save_dir.joinpath(im_segmentation_name)}.png'
  if(pathlib.Path(im_segmentation_path).exists()):
    continue

  # # Create the image segmenter
  # with vision.ImageSegmenter.create_from_options(options) as segmenter:
  #   # Create the MediaPipe image file that will be segmented
  #   image = mp.Image.create_from_file(f'{im_path}')
  #   # Retrieve the masks for the segmented image
  #   segmentation_result = segmenter.segment(image)
  #   category_mask = segmentation_result.category_mask
    
  #   # Generate solid color images for showing the output segmentation mask.
  #   image_data = image.numpy_view()
  #   fg_image = np.zeros(image_data.shape, dtype=np.uint8)
  #   fg_image[:] = MASK_COLOR
  #   bg_image = np.zeros(image_data.shape, dtype=np.uint8)
  #   bg_image[:] = BG_COLOR

  #   condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.2
  #   output_image = np.where(condition, fg_image, bg_image)

  #   cv2.imwrite(im_segmentation_path, output_image)
  #   
  im = cv2.imread(f'{im_path.resolve()}')
  original_output = predictor(im)
  specific_output, mask_image, more_than_one_mask = onlykeep_person_class(im, original_output)  

  v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)  
  out = v.draw_instance_predictions(specific_output.to("cpu"))
  # cv2.imwrite(im_segmentation_path, out.get_image()[:, :, ::-1])
  cv2.imwrite(im_segmentation_path, mask_image)